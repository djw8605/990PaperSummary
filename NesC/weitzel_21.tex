\documentclass[12pt]{article}


\begin{document}

\begin{center}
{\huge Summary: The nesC Language} \\
Derek Weitzel
\end{center}

This paper is a very in-depth look at the programming language nesC.  This paper was written in 2003, which explains the extreme reliance on explaining concurrency.  

This paper writes a lot about concurrency.  This is likely because it was written in 2003, before major investment in large scale concurrency was mainstream.  But today, concurrency is everywhere, and well understood. For example, the consumer operating systems Mac OSX and iOS, there is an framework call grand central dispatch.  It is used to provide task like concurrency very similar to the task like concurrency in the paper.  If written today, I don't believe so much of this paper would focus on concurrency.

The paper claims that the TinyOS is different from other OS's in that it is driven by the environment.  I would claim that all computers are event driven.  For example, in a cluster, an event can be a user submitting a job.  A worker node will respond to the event by running the job.  In a desktop, an event can be a user interaction.  Or an event can be a network packet (same as TinyOS).  It seems odd that the authors do not see modern computing as event driven.  Even if the `tasks' in traditional OS's may be longer than those in TinyOS, they are still started by an event.

The authors noted that the target price for a mote is \$0.10 per mote.  This seems optimistic, and possibly a way in the future.  I suspect that hardware will need to change substantially in order for this price point to be achieved.  So much so, that this TinyOS will likely not be viable to run on it.

The paper makes a point that there is no dynamic memory allocation in NesC, which TinyOS is written in.  If that is true, then how does the scheduler keep track of all the tasks in the run queue?  Is there a maximum limit of tasks allowed to be in the queue?  Is there clever compile time optimization?  The authors never specify.

The programming language is strikingly similar to VHDL.  Even similar names, interfaces and implementations.  It's not clear to me that the authors are correct in their statement that nesC configurations do not encapsulate the components they use.  They may not have to, but it's hard to imagine a scenario where a developer would not encapsulate a component to make it useful.  And concurrency in VHDL is extreme, across component boundaries, inside components, everywhere.

\end{document}

